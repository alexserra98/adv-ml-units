{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: **Invariance** and **Equivariance** at different layers of a *CNN*\n",
    "\n",
    "Advanced Topics in Machine Learning -- Spring 2023, UniTS\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ganselmif/adv-ml-units/blob/main/notebooks/AdvML_UniTS_2023_Lab_05_CNN_Invariance_Equivariance.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of the *Lab*\n",
    "\n",
    "In the following *Lab*, we will study the *invariance* and *equivariance* properties of specific layers within a *CNN*.\n",
    "\n",
    "Recall the definitions -- respectively -- of **invariance** and **equivariance** of (the result of) function $f$ with respect to transformation (expressed in the form of an operator) $P_{\\alpha}$ parametrized by $\\alpha$:\n",
    "\n",
    "- *Invariance*: $f(P_{\\alpha} x) = f(x)\\;\\;\\;\\; \\forall\\alpha$\n",
    "- *Equivariance*: $f(P_{\\alpha} x) = P_{\\alpha} f(x)\\;\\;\\;\\; \\forall\\alpha$\n",
    "\n",
    "According to theory, the training of a *CNN* with pooling should lead to a network whose:\n",
    "\n",
    "- *Convolutional* layers are *equivariant* to traslation;\n",
    "- *Fully Connected* layers are *invariant* to traslation.\n",
    "\n",
    "Due to the specific structure of convolutional layers, it is possible to show that the *equivariance* property gives rise to permuted activations in response to translation of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following *CNN* model is given, whose output -- for your convenience -- is a tuple, composed of the actual output of the network, the activation tensor after the second *convolutional* layer, and the activation tensor after the first *fully-connected* layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        conv2repr = x.clone().detach()\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, 320)\n",
    "        x = self.fc1(x)\n",
    "        fc1repr = x.clone().detach()\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x, conv2repr, fc1repr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking inspiration from previous *Labs*:\n",
    "\n",
    "1. Train the model on the (non-augmented) *MNIST* dataset;\n",
    "2. Prepare a test dataset composed of pairs of mutually traslated images;\n",
    "3. Extract the activations of layers `conv2` and `fc1` and check whether they respect the invariance/equivariance property.\n",
    "\n",
    "**Hint**: To test for *equivariance*, it may be useful to notice that **sorting** is invariant to permutations!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train the model on the (non-augmented) *MNIST* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.0\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining transforms\n",
    "augmentation = transforms.RandomAffine(degrees=(0, 180), translate=None, scale=None)\n",
    "to_tensor = transforms.ToTensor()\n",
    "normalization = transforms.Normalize(mean=0.1307, std=0.3081)\n",
    "\n",
    "# Loading the datasets\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    transform=to_tensor,\n",
    "    download=True,\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    transform=to_tensor,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5  # Number of times the whole (training) dataset is used for training\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, _, _ = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "for i, (images,labels) in enumerate(test_loader):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs, _, _ = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "acc = 100 * correct / total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare a test dataset composed of pairs of mutually traslated images;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining transforms\n",
    "augmentation = transforms.RandomAffine(degrees=(30, 70), translate=(0.1, 0.3), scale=None)\n",
    "to_tensor = transforms.ToTensor()\n",
    "normalization = transforms.Normalize(mean=0.1307, std=0.3081)\n",
    "\n",
    "# Loading the datasets\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    transform=transforms.Compose([augmentation, to_tensor, normalization])\n",
    "    download=True,\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    transform=transforms.Compose([augmentation, to_tensor, normalization])\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPkibvpTEMRILBn2/x8IuJj",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
